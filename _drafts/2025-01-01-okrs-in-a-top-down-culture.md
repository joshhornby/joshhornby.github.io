---
title: "From Leading to Lagging: Rethinking Outcome Measurement"
date: 2025-08-07 08:00:00
tags: [culture, product]
sitemap:
    priority: 0.7
    changefreq: 'monthly'
    lastmod: "2025-08-07 T19:00:00+01:00"
---

Last year, I watched a product team celebrate hitting their OKR targets. They'd increased user engagement by 23%, reduced churn by 8%, and shipped twelve major features. Three months later, the business unit was restructured because revenue had tanked. Nobody saw it coming—except they should have. Every metric they tracked was a lagging indicator, telling them what had already happened instead of what was about to.

The problem runs deeper than bad metrics. We've turned measurement into theatre. Teams perform the rituals—weekly dashboards, monthly reviews, quarterly retrospectives—but miss the fundamental distinction between metrics that report history and metrics that predict the future.

Here's what happens in most top-down organisations: leadership decides what to build, sets deadlines, then asks teams to find metrics that justify the plan. It's backwards. The first question should be "what outcome moves the business forward?" Only then do you work out what to build.

Watch how teams discuss metrics in practice. They'll show you last quarter's revenue (lagging), this month's user growth (lagging), yesterday's engagement scores (lagging). Where are the forward-looking signals? Where's the metric that tells you in week 2 whether you'll hit your Q4 target? Most teams can't answer because they never built that predictive muscle.

This happens because incentives trump process every time. If bonuses depend on shipping features, not business outcomes, then outcome measurement becomes another box to tick. The metrics exist, but nobody acts on them.

I once worked with a team tracking 47 different metrics. When I asked which ones predicted revenue, they couldn't tell me. They'd never tested the relationships. They just tracked what leadership asked for. Finding real leading indicators requires understanding causality: if X moves today, Y will move next month. Most teams never do this analysis.

The gap between process and practice is where good intentions die. Your wiki might say "challenge assumptions with data", but if the unspoken rule is "don't question the roadmap", then outcomes become another compliance exercise. The real process is what people do when nobody's watching.

Even in command-and-control cultures, working backwards from outcomes creates leverage. When you start with "we need to increase retention by 15%" instead of "we need to build feature X", you force different conversations. Suddenly teams ask: Will this feature actually move retention? What early signal tells us if we're on track? These questions expose hidden assumptions.

A clear outcome metric gives teams permission to push back. "This feature won't move our retention target" carries more weight than "I don't think we should build this." The conversation shifts from activity to impact.

Good metrics create productive friction. If your outcome measures don't provoke debate, they're probably not measuring anything meaningful. The discomfort means you're surfacing real trade-offs, not just tracking vanity metrics.

None of this works if teams are running at 120% capacity. Finding leading indicators takes time. Testing causality takes experimentation. If you're always scrambling to deliver, measurement becomes another burden instead of a tool for better decisions.

You won't transform a top-down culture overnight. But you can use measurement to shift conversations:

- Start with lagging indicators that directly measure business success.
- Identify the leading indicators that predict those outcomes.
- Map every initiative to both leading and lagging measures.
- Focus daily and weekly discussions on leading indicators.
- Use monthly or quarterly reviews to validate if leading indicators actually predicted outcomes.
- Reward teams that find better leading indicators, not just those who hit targets.

## Real-world Examples of Outcome Measurement

Here are two contrasting examples from my experience:

**Example 1: The Dashboard Nobody Looked At**

At a fintech startup, the CEO mandated building a "comprehensive analytics dashboard" for enterprise clients. The team dutifully tracked adoption metrics: 85% of clients had activated the feature, average session time was 12 minutes. Lagging indicators looked great.

Six months later, during renewal discussions, we discovered the truth. Clients logged in once during onboarding, poked around, then never returned. The feature they actually wanted was automated reporting delivered to their inbox. We'd built the wrong thing, but our metrics hid the failure.

The leading indicator we should have tracked: repeat usage within 30 days. That would have shown the problem in week 5, not month 6.

**Example 2: The Metric That Predicted Churn**

At cinch, our product team noticed customers who didn't complete their first vehicle inspection within 7 days had 3x higher churn rates. This became our north star leading indicator. Every feature was evaluated on whether it moved this metric.

When leadership pushed for a premium subscription tier, we ran the numbers. Premium features wouldn't help customers complete inspections faster. We proposed investing in onboarding improvements instead. The data was so clear that even our command-and-control leadership agreed.

Result: 40% reduction in early churn, worth £2.1M in annual revenue.

## Framework for Metric Selection

When choosing metrics, work through these questions:

1. **Causality Check**

- Does the metric actually predict the outcome you care about?
- Can you validate the relationship with historical data?
- What's your confidence interval on the prediction?

2. **Actionability Assessment**

- Can teams influence this metric directly?
- How quickly can you see the impact of changes?
- Are there obvious gaming risks?

3. **Implementation Viability**

- Can you measure this reliably?
- What's the cost of collecting and processing the data?
- How quickly can you detect significant changes?

At cinch, we tracked "support ticket volume" as a proxy for customer satisfaction. Then ticket numbers dropped 30% in a month. Success? No—customers had given up asking for help. We switched to tracking "time to first resolution" and "self-service success rate". These actually predicted renewal rates.

## When to Adjust Your Metrics

Most organisations cling to metrics long after they've stopped being useful. Time to change when:

1. **Your leading indicators stop predicting outcomes**

- Your product or user base has evolved significantly
- Market conditions have changed fundamentally
- You've optimised the metric to the point of diminishing returns

2. **Teams game the metrics instead of solving problems**

- Solutions designed around the metric rather than the problem
- Declining secondary metrics that weren't being tracked
- Teams feel constrained by metrics rather than guided by them

3. **The business has moved on but metrics haven't**

- New strategic initiatives require different success criteria
- Competitive landscape changes demand new focus areas
- Your metrics measure problems you've already solved

Outcome measurement won't fix a broken culture. But it makes the dysfunction visible. When everyone can see that Feature X didn't move the needle, it's harder to pretend activity equals progress. Use metrics as a wedge to open conversations about what actually matters. That's how you start shifting from "ship it because the boss said so" to "ship it because it works."
