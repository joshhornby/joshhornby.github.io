---
title: "The Many Faces of Software Quality"
date: 2025-08-07 08:00:00
tags: [software-engineering]
sitemap:
    priority: 0.7
    changefreq: 'monthly'
    lastmod: "2025-08-07 T19:00:00+01:00"
---

Product managers see quality through user satisfaction and bug counts. Engineers see it through code maintainability and system reliability. These different views create friction.

Walk into any retrospective after a messy release and you'll hear the same conversation. Product asks why there were so many bugs. Engineering explains they hit all their test coverage targets and the code is clean. Product points to angry customer emails. Engineering points to green CI builds. Everyone's right, everyone's frustrated.

The problem isn't that one side is wrong. It's that quality means different things to different people, and most teams never acknowledge this fundamental mismatch.

## The Quality Perception Gap

When product teams ask, "Why do we have more bugs?" they're often looking at quality through metrics like:

- User-reported issues
- Feature completeness
- Customer satisfaction scores
- App store ratings
- Support ticket volume

Meanwhile, engineering teams are considering:

- Code coverage percentages
- System uptime
- Response latency
- Technical debt measurements
- Code maintainability scores

This disconnect reflects different ways of thinking about quality.

## Weinberg's Quality Frameworks

Gerald Weinberg identified five ways people view quality:

1. **Transcendent Quality**: The "I know it when I see it" perspective. Common among experienced people but hard to measure.
2. **User-based Quality**: Does the software do what users need? Product managers typically think this way.
3. **Manufacturing-based Quality**: Conformance to specifications. Engineers like this because it's measurable.
4. **Product-based Quality**: Quality from inherent characteristics like performance or reliability.
5. **Value-based Quality**: Benefits against costs. Quality as return on investment.

## Bridging the Divide

So how do you get everyone talking about the same thing when they use the word "quality"?

Start with metrics that both sides care about. Instead of arguing over test coverage percentages or customer satisfaction scores in isolation, find the overlap. Time to resolve critical issues matters to everyone, product because users are affected, engineering because it reflects system stability. User-impacting incident frequency tells you about both code quality and user experience. Feature adoption rates reveal whether you're building the right thing well.

The trick is getting both perspectives in the room together. Quality reviews that include only engineers will optimise for technical concerns. Reviews with only product people will miss underlying systemic issues. You need both viewpoints arguing with each other, not past each other.

This means developing a shared language around quality. What's the difference between a bug and a feature request? When is anything critical versus urgent? These seem like semantic debates, but they matter when you're trying to prioritise fixes or understand quality trends. 

And stop having subjective arguments about whether quality is "getting better" or "getting worse." Track trends over time. Set measurable goals. Use data to drive decisions, not feelings or anecdotes from the loudest person in the room.

## Moving Forward

Quality isn't one-dimensional. Understanding different perspectives helps teams build better approaches.

Don't force everyone into the same view of quality. Create frameworks that acknowledge different perspectives. Multiple quality metrics, different reviews for different parts.

Quality evolves with your product. Reassess metrics and goals regularly.

Acknowledge different perspectives. Build systems that account for them. Better software, healthier teams.


